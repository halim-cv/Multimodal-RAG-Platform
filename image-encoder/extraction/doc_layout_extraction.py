# -*- coding: utf-8 -*-
"""Doc_Layout_Extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19I7_AEHuQRFko4cXIRDeo8i4JpbjQufG
"""

!pip install doclayout-yolo

import cv2
from doclayout_yolo import YOLOv10
from huggingface_hub import hf_hub_download # Import hf_hub_download

# Download a fine-tuned model weights from Hugging Face Hub
# The file 'doclayout_yolo_docstructbench_imgsz1024.pt' is present in the root of the 'juliozhao/DocLayout-YOLO-DocStructBench' repository
model_path = hf_hub_download(repo_id="juliozhao/DocLayout-YOLO-DocStructBench", filename="doclayout_yolo_docstructbench_imgsz1024.pt")

# Initialize YOLOv10 with the path to the downloaded model
model = YOLOv10(model=model_path)

!pip install PyMuPDF

import fitz  # PyMuPDF
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt

def pdf_to_image(pdf_path, page_number=0, dpi=200):
    doc = fitz.open(pdf_path)
    page = doc.load_page(page_number)
    pix = page.get_pixmap(matrix=fitz.Matrix(dpi / 72, dpi / 72))
    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
    doc.close()
    return np.array(img)

pdf_path = '/content/HighlightedV1.pdf'

# Convert the fourth page of the PDF to an image (page_number=3 for the 4th page)
image_array = pdf_to_image(pdf_path, page_number=3)

# Display the image to verify
plt.figure(figsize=(10, 10))
plt.imshow(image_array)
plt.axis('off')
plt.title(f"Page 4 of {pdf_path}")
plt.show()

print(f"Successfully converted page 4 of {pdf_path} to an image with shape: {image_array.shape}")

# Run inference on the image array
results = model(image_array)

# The results object usually contains detected objects, which can be plotted directly.
# The 'doclayout-yolo' model returns a list of result objects, one for each image (even if there's only one).
# We can access the first (and only) result for our single image.
result = results[0]

# Plot the results on the original image. The plot method returns an image with detections drawn.
annotated_image = result.plot(labels=True, conf=False) # labels=True to show class names, conf=False to hide confidence scores

# Convert the annotated image (which is a BGR numpy array from OpenCV) to RGB for matplotlib
annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)

# Display the annotated image
plt.figure(figsize=(15, 15))
plt.imshow(annotated_image_rgb)
plt.axis('off')
plt.title('YOLOv10 Document Layout Detection')
plt.show()

# Run inference on the image array
results = model(image_array)

# The results object usually contains detected objects, which can be plotted directly.
# The 'doclayout-yolo' model returns a list of result objects, one for each image (even if there's only one).
# We can access the first (and only) result for our single image.
result = results[0]

# Plot the results on the original image. The plot method returns an image with detections drawn.
annotated_image = result.plot(labels=True, conf=False) # labels=True to show class names, conf=False to hide confidence scores

# Convert the annotated image (which is a BGR numpy array from OpenCV) to RGB for matplotlib
annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)

# Display the annotated image
plt.figure(figsize=(15, 15))
plt.imshow(annotated_image_rgb)
plt.axis('off')
plt.title('YOLOv10 Document Layout Detection')
plt.show()

"""# Task
Extract figure and caption bounding boxes and labels from the YOLOv10 inference results for the PDF page in `image_array`, associate figures with their nearest captions using spatial proximity, crop and store the figure-caption pairs as image regions, and finally display a few of these extracted pairs to verify the process.

## Extract Bounding Boxes and Labels

### Subtask:
Retrieve the bounding boxes, class IDs, and class names for all detected objects from the YOLOv10 inference results.

**Reasoning**:
The subtask requires extracting bounding boxes, class IDs, and class names from the YOLOv10 inference results. The provided instructions detail how to access these attributes from the `result` object and organize them into a structured format.
"""

import pandas as pd

# Access the result object, which is the first (and only) element in the results list
result = results[0]

# Extract bounding box coordinates (xyxy format) and class IDs
bboxes = result.boxes.xyxy.cpu().numpy() # Convert to numpy array if it's a torch tensor
class_ids = result.boxes.cls.cpu().numpy() # Convert to numpy array

# Get the dictionary mapping class IDs to class names
class_names_map = result.names

# Prepare a list to store the extracted data
detected_objects_data = []

# Iterate through each detected object and extract its information
for i in range(len(bboxes)):
    bbox = bboxes[i].tolist() # Convert numpy array to list
    class_id = int(class_ids[i])
    class_name = class_names_map[class_id]

    detected_objects_data.append({
        'bbox': bbox,
        'class_id': class_id,
        'class_name': class_name
    })

# Convert the list of dictionaries to a Pandas DataFrame for better organization and analysis
detected_objects_df = pd.DataFrame(detected_objects_data)

# Display the first few rows of the DataFrame to verify
print("Detected Objects DataFrame:")
print(detected_objects_df.head())

print(f"Successfully extracted data for {len(detected_objects_df)} detected objects.")

"""## Associate Figures with Captions

### Subtask:
Develop a strategy to associate each 'figure' detection with its corresponding 'figure_caption'. This will likely involve spatial proximity analysis, such as finding the nearest caption below each figure.

**Reasoning**:
The first step is to filter the `detected_objects_df` to separate figures and figure captions into their respective DataFrames, as specified in the instructions. This prepares the data for the association process.
"""

import numpy as np

# 1. Filter the detected_objects_df DataFrame for 'figure' and 'figure_caption' objects
figures_df = detected_objects_df[detected_objects_df['class_name'] == 'figure'].copy()
captions_df = detected_objects_df[detected_objects_df['class_name'] == 'figure_caption'].copy()

print(f"Found {len(figures_df)} figures and {len(captions_df)} figure captions.")

# Display the first few rows of the filtered DataFrames to verify
print("\nFigures DataFrame:")
print(figures_df.head())

print("\nCaptions DataFrame:")
print(captions_df.head())

"""**Reasoning**:
Now that the figures and captions are separated, the next step is to define a function to calculate the vertical and horizontal distances between a figure and a caption's bounding boxes. This function will be crucial for determining spatial proximity as required by the instructions.


"""

def calculate_distance(figure_bbox, caption_bbox, image_height):
    # figure_bbox: [x_min, y_min, x_max, y_max]
    # caption_bbox: [x_min, y_min, x_max, y_max]

    fig_x_min, fig_y_min, fig_x_max, fig_y_max = figure_bbox
    cap_x_min, cap_y_min, cap_x_max, cap_y_max = caption_bbox

    # Vertical distance: distance from bottom of figure to top of caption
    # Only consider captions below the figure
    if cap_y_min > fig_y_max:
        vertical_distance = cap_y_min - fig_y_max
    else:
        # Caption is not below the figure, assign a very large distance
        return float('inf'), float('inf')

    # Horizontal overlap/distance
    # Calculate intersection over union (IoU) for x-axis or distance if no overlap
    x_overlap = max(0, min(fig_x_max, cap_x_max) - max(fig_x_min, cap_x_min))
    fig_width = fig_x_max - fig_x_min
    cap_width = cap_x_max - cap_x_min

    if x_overlap > 0: # There is horizontal overlap
        horizontal_proximity = 0 - x_overlap # Prioritize larger overlap, so smaller negative value is better
    else: # No horizontal overlap, calculate distance
        # Distance between right of figure and left of caption OR left of figure and right of caption
        dist1 = abs(fig_x_max - cap_x_min)
        dist2 = abs(fig_x_min - cap_x_max)
        horizontal_proximity = min(dist1, dist2)

    return vertical_distance, horizontal_proximity

# Prepare to store associated pairs
associated_pairs = []

captions_available = captions_df.copy()

# Iterate through each figure
for fig_index, figure in figures_df.iterrows():
    figure_bbox = figure['bbox']
    best_caption = None
    min_vertical_dist = float('inf')
    min_horizontal_proximity = float('inf')
    best_caption_index = -1

    # Iterate through available captions to find the best match
    for cap_index, caption in captions_available.iterrows():
        caption_bbox = caption['bbox']

        vertical_dist, horizontal_prox = calculate_distance(figure_bbox, caption_bbox, image_array.shape[0])

        # Conditions for considering a caption:
        # 1. Caption must be below the figure (handled by calculate_distance returning inf)
        # 2. Vertical distance is smaller than current best_vertical_dist
        # 3. If vertical distances are equal, horizontal_proximity is better (smaller)
        if vertical_dist < min_vertical_dist:
            min_vertical_dist = vertical_dist
            min_horizontal_proximity = horizontal_prox
            best_caption = caption
            best_caption_index = cap_index
        elif vertical_dist == min_vertical_dist and horizontal_prox < min_horizontal_proximity:
            min_horizontal_proximity = horizontal_prox
            best_caption = caption
            best_caption_index = cap_index

    if best_caption is not None:
        associated_pairs.append({
            'figure_bbox': figure_bbox,
            'figure_class_name': figure['class_name'],
            'caption_bbox': best_caption['bbox'],
            'caption_class_name': best_caption['class_name'],
            'vertical_distance': min_vertical_dist,
            'horizontal_proximity': min_horizontal_proximity
        })
        # Remove the associated caption from available captions to prevent re-association
        captions_available = captions_available.drop(best_caption_index)


# Print a summary of the associated pairs
print("\nAssociated Figure-Caption Pairs:")
if associated_pairs:
    for i, pair in enumerate(associated_pairs):
        print(f"Pair {i+1}:")
        print(f"  Figure BBox: {pair['figure_bbox']}")
        print(f"  Caption BBox: {pair['caption_bbox']}")
        print(f"  Vertical Distance: {pair['vertical_distance']:.2f}")
        print(f"  Horizontal Proximity: {pair['horizontal_proximity']:.2f}")
else:
    print("No figure-caption pairs associated.")

print(f"\nSuccessfully associated {len(associated_pairs)} figure-caption pairs.")

"""**Reasoning**:
Now that figure-caption pairs are associated, the next step is to crop these regions from the original image and display them. This involves iterating through the `associated_pairs`, extracting the bounding box coordinates for each figure and caption, cropping them from `image_array`, and then displaying a few to verify the process.


"""

extracted_figure_caption_images = []

for i, pair in enumerate(associated_pairs):
    figure_bbox = [int(coord) for coord in pair['figure_bbox']]
    caption_bbox = [int(coord) for coord in pair['caption_bbox']]

    # Crop the figure image
    fig_x_min, fig_y_min, fig_x_max, fig_y_max = figure_bbox
    figure_image = image_array[fig_y_min:fig_y_max, fig_x_min:fig_x_max]

    # Crop the caption image
    cap_x_min, cap_y_min, cap_x_max, cap_y_max = caption_bbox
    caption_image = image_array[cap_y_min:cap_y_max, cap_x_min:cap_x_max]

    # Stack them vertically for display. Pad if widths are different.
    # Get max width for padding
    max_width = max(figure_image.shape[1], caption_image.shape[1])

    # Pad figure image if its width is smaller
    if figure_image.shape[1] < max_width:
        pad_width = max_width - figure_image.shape[1]
        figure_image = np.pad(figure_image, ((0, 0), (0, pad_width), (0, 0)), mode='constant', constant_values=255)

    # Pad caption image if its width is smaller
    if caption_image.shape[1] < max_width:
        pad_width = max_width - caption_image.shape[1]
        caption_image = np.pad(caption_image, ((0, 0), (0, pad_width), (0, 0)), mode='constant', constant_values=255)

    # Combine figure and caption vertically
    combined_image = np.vstack((figure_image, np.full((20, max_width, 3), 255), caption_image)) # Add a white separator
    extracted_figure_caption_images.append(combined_image)

# Display a few of the extracted pairs
print(f"Displaying {min(3, len(extracted_figure_caption_images))} extracted figure-caption pairs...")
num_to_display = min(3, len(extracted_figure_caption_images))

for i in range(num_to_display):
    plt.figure(figsize=(10, 10))
    plt.imshow(extracted_figure_caption_images[i])
    plt.axis('off')
    plt.title(f'Extracted Figure-Caption Pair {i+1}')
    plt.show()

print(f"Successfully cropped and stored {len(extracted_figure_caption_images)} figure-caption pairs.")

"""## Final Task

### Subtask:
Summarize the process of extracting figure-caption pairs and present the data structure containing these pairs.

## Summary:

### Data Analysis Key Findings
*   A total of 18 objects were detected by the YOLOv10 model, including instances of 'figure' and 'figure\_caption' classes.
*   Specifically, the process identified 1 'figure' and 1 'figure\_caption' from the detected objects.
*   A spatial proximity algorithm was successfully implemented to associate figures with their captions. For the identified pair, the vertical distance from the figure's bottom to the caption's top was approximately 28.07 pixels, and a horizontal proximity value of -530.63 pixels indicated significant horizontal overlap.
*   The associated figure and caption were successfully cropped from the original image and combined into a single image region, which was then displayed, confirming the accuracy of the extraction and association process.

### Insights or Next Steps
*   The current figure-caption association method, based on nearest vertical and horizontal proximity, works effectively for simple layouts. For more complex document structures (e.g., multi-column layouts, figures spanning multiple pages, or captions placed above figures), refining the spatial proximity logic to incorporate additional contextual cues like column boundaries or reading order could enhance accuracy.
*   The extracted figure-caption image pairs are now ready for subsequent processing. This could involve applying Optical Character Recognition (OCR) to the caption image to extract text for indexing, searching, or semantic analysis, or further image processing on the figure itself for content understanding or feature extraction.

# Task
Iterate through all pages of the `HighlightedV1.pdf` document. For each page, convert it to an image, perform YOLOv10 inference, and collect all detection results (bounding boxes, class IDs, class names) along with their corresponding page numbers. Store these consolidated results in a single Pandas DataFrame.

## Iterate through PDF pages and perform inference

### Subtask:
Iterate through all pages of the `HighlightedV1.pdf` document. For each page, convert it to an image and run the YOLOv10 model. Collect all detection results, along with their respective page numbers, for subsequent processing.

**Reasoning**:
The subtask requires iterating through all pages of the PDF, converting each page to an image, running YOLOv10 inference, and collecting all detection results along with their page numbers into a Pandas DataFrame. The provided instructions detail each step to achieve this.
"""

import fitz  # PyMuPDF
import numpy as np
from PIL import Image
import pandas as pd

# 1. Open the PDF document
doc = fitz.open(pdf_path)

# 2. Get the total number of pages
total_pages = doc.page_count
print(f"Total pages in PDF: {total_pages}")

# 3. Initialize an empty list to store all detection data
all_detections_data = []

# 4. Loop through each page
for page_number in range(total_pages):
    print(f"Processing page {page_number + 1}/{total_pages}...")
    # 5.a. Load the current page
    page = doc.load_page(page_number)

    # 5.b. Convert the page to a pixmap with a suitable DPI
    dpi = 200 # Using 200 DPI as in previous steps
    pix = page.get_pixmap(matrix=fitz.Matrix(dpi / 72, dpi / 72))

    # 5.c. Convert the pixmap to a PIL Image and then to a NumPy array
    image_array = np.array(Image.frombytes("RGB", [pix.width, pix.height], pix.samples))

    # 5.d. Perform YOLOv10 inference
    results_page = model(image_array)

    # 5.e. Access the first (and only) result object for the current page
    result = results_page[0]

    # 5.f. Extract bounding boxes and class IDs
    if result.boxes is not None and len(result.boxes) > 0:
        bboxes = result.boxes.xyxy.cpu().numpy()
        class_ids = result.boxes.cls.cpu().numpy()

        # 5.g. Get the dictionary mapping class IDs to class names
        class_names_map = result.names

        # 5.h. Iterate through each detected object on the current page
        for i in range(len(bboxes)):
            bbox = bboxes[i].tolist()
            class_id = int(class_ids[i])
            class_name = class_names_map[class_id]

            # 5.i. Create a dictionary and append to the list
            all_detections_data.append({
                'page_number': page_number,
                'bbox': bbox,
                'class_id': class_id,
                'class_name': class_name
            })
    else:
        print(f"No detections found on page {page_number + 1}.")

# 6. Close the PDF document
doc.close()

# 7. Convert the list of dictionaries to a Pandas DataFrame
all_detections_df = pd.DataFrame(all_detections_data)

# 8. Display the first few rows and shape of the DataFrame
print("\nAll Detections DataFrame (head):")
print(all_detections_df.head())
print(f"\nShape of all_detections_df: {all_detections_df.shape}")

"""## Separate figures/tables and their captions

### Subtask:
Filter the consolidated DataFrame (`all_detections_df`) to create distinct DataFrames for 'figure', 'figure_caption', 'table', and 'table_caption' objects.

**Reasoning**:
The subtask requires filtering the `all_detections_df` to create distinct DataFrames for 'figure', 'figure_caption', 'table', and 'table_caption' objects, and then printing the counts of each.
"""

figures_df = all_detections_df[all_detections_df['class_name'] == 'figure'].copy()
figure_captions_df = all_detections_df[all_detections_df['class_name'] == 'figure_caption'].copy()
tables_df = all_detections_df[all_detections_df['class_name'] == 'table'].copy()
table_captions_df = all_detections_df[all_detections_df['class_name'] == 'table_caption'].copy()

print(f"Found {len(figures_df)} figures.")
print(f"Found {len(figure_captions_df)} figure captions.")
print(f"Found {len(tables_df)} tables.")
print(f"Found {len(table_captions_df)} table captions.")

"""## Associate figures with figure captions

### Subtask:
Adapt the spatial proximity logic to associate 'figure' detections with their nearest 'figure_caption' detections, prioritizing captions on the same page and closest in vertical proximity, ensuring each caption is used only once.

**Reasoning**:
The subtask requires adapting the spatial proximity logic to associate 'figure' detections with their nearest 'figure_caption' detections across multiple pages, prioritizing captions on the same page and closest in vertical proximity, and ensuring each caption is used only once. This step defines the `calculate_distance` function which is essential for determining spatial proximity, initializes the list to store associated pairs, and creates a working copy of the captions DataFrame.
"""

def calculate_distance(figure_bbox, caption_bbox):
    # figure_bbox: [x_min, y_min, x_max, y_max]
    # caption_bbox: [x_min, y_min, x_max, y_max]

    fig_x_min, fig_y_min, fig_x_max, fig_y_max = figure_bbox
    cap_x_min, cap_y_min, cap_x_max, cap_y_max = caption_bbox

    # Vertical distance: distance from bottom of figure to top of caption
    # Only consider captions below the figure and above the figure
    if cap_y_min > fig_y_max: # Caption is below the figure
        vertical_distance = cap_y_min - fig_y_max
        vertical_direction = 'below'
    elif cap_y_max < fig_y_min: # Caption is above the figure
        vertical_distance = fig_y_min - cap_y_max
        vertical_direction = 'above'
    else:
        # Caption overlaps vertically with the figure, which is generally not a caption for it
        return float('inf'), float('inf'), 'overlap'

    # Horizontal overlap/distance
    x_overlap = max(0, min(fig_x_max, cap_x_max) - max(fig_x_min, cap_x_min))

    if x_overlap > 0: # There is horizontal overlap
        horizontal_proximity = 0 - x_overlap # Prioritize larger overlap, so smaller negative value is better
    else: # No horizontal overlap, calculate distance
        # Distance between right of figure and left of caption OR left of figure and right of caption
        dist1 = abs(fig_x_max - cap_x_min)
        dist2 = abs(fig_x_min - cap_x_max)
        horizontal_proximity = min(dist1, dist2)

    return vertical_distance, horizontal_proximity, vertical_direction

# Initialize an empty list to store associated figure-caption pairs
associated_figure_caption_pairs = []

# Create a working copy of figure_captions_df to keep track of available captions
available_figure_captions_df = figure_captions_df.copy()

print(f"Initial number of available figure captions: {len(available_figure_captions_df)}")
print(f"Initial number of figures to process: {len(figures_df)}")

"""**Reasoning**:
Now, the next step is to iterate through each figure, identify captions on the same page, and find the best matching caption based on spatial proximity (vertical distance then horizontal proximity). Once a caption is associated, it should be removed from the list of available captions to ensure it's used only once, and the association details are stored.


"""

for fig_index, figure in figures_df.iterrows():
    figure_bbox = figure['bbox']
    figure_page = figure['page_number']

    # Filter available captions for the current page
    same_page_captions = available_figure_captions_df[
        available_figure_captions_df['page_number'] == figure_page
    ]

    best_caption_for_figure = None
    min_vertical_dist_for_figure = float('inf')
    min_horizontal_proximity_for_figure = float('inf')
    best_caption_row_index = -1

    # Iterate through same-page captions to find the best match
    for cap_row_index, caption in same_page_captions.iterrows():
        caption_bbox = caption['bbox']

        vertical_dist, horizontal_prox, vertical_direction = calculate_distance(figure_bbox, caption_bbox)

        # Only consider captions that are 'below' the figure and have a valid distance
        if vertical_direction == 'below' and vertical_dist != float('inf'):
            if vertical_dist < min_vertical_dist_for_figure:
                min_vertical_dist_for_figure = vertical_dist
                min_horizontal_proximity_for_figure = horizontal_prox
                best_caption_for_figure = caption
                best_caption_row_index = cap_row_index
            elif vertical_dist == min_vertical_dist_for_figure and horizontal_prox < min_horizontal_proximity_for_figure:
                # If vertical distances are equal, prioritize smaller horizontal proximity (larger overlap if negative)
                min_horizontal_proximity_for_figure = horizontal_prox
                best_caption_for_figure = caption
                best_caption_row_index = cap_row_index

    if best_caption_for_figure is not None:
        associated_figure_caption_pairs.append({
            'figure_page_number': figure['page_number'],
            'figure_bbox': figure_bbox,
            'figure_class_name': figure['class_name'],
            'caption_page_number': best_caption_for_figure['page_number'],
            'caption_bbox': best_caption_for_figure['bbox'],
            'caption_class_name': best_caption_for_figure['class_name'],
            'vertical_distance': min_vertical_dist_for_figure,
            'horizontal_proximity': min_horizontal_proximity_for_figure
        })
        # Remove the associated caption from available captions to prevent re-association
        available_figure_captions_df = available_figure_captions_df.drop(best_caption_row_index)

# Print a summary of the associated pairs
print("\nAssociated Figure-Caption Pairs:")
if associated_figure_caption_pairs:
    for i, pair in enumerate(associated_figure_caption_pairs):
        print(f"Pair {i+1}:")
        print(f"  Figure Page: {pair['figure_page_number']}")
        print(f"  Figure BBox: {pair['figure_bbox']}")
        print(f"  Caption Page: {pair['caption_page_number']}")
        print(f"  Caption BBox: {pair['caption_bbox']}")
        print(f"  Vertical Distance: {pair['vertical_distance']:.2f}")
        print(f"  Horizontal Proximity: {pair['horizontal_proximity']:.2f}")
else:
    print("No figure-caption pairs associated.")

print(f"\nSuccessfully associated {len(associated_figure_caption_pairs)} figure-caption pairs.")

"""## Associate tables with table captions

### Subtask:
Implement a similar spatial proximity logic to associate 'table' detections with their corresponding 'table_caption' detections, following the same principles as the figure-caption association.

**Reasoning**:
The subtask requires iterating through each table, identifying captions on the same page, and finding the best matching caption based on spatial proximity (vertical distance then horizontal proximity), considering both above and below positions. Once a caption is associated, it should be removed from the list of available captions to ensure it's used only once, and the association details are stored.
"""

associated_table_caption_pairs = []
available_table_captions_df = table_captions_df.copy()

print(f"Initial number of available table captions: {len(available_table_captions_df)}")
print(f"Initial number of tables to process: {len(tables_df)}")

for table_index, table in tables_df.iterrows():
    table_bbox = table['bbox']
    table_page = table['page_number']

    # Filter available captions for the current page
    same_page_captions = available_table_captions_df[
        available_table_captions_df['page_number'] == table_page
    ]

    best_caption_for_table = None
    min_total_proximity = float('inf') # Combine vertical and horizontal for comparison
    best_caption_row_index = -1

    # Iterate through same-page captions to find the best match
    for cap_row_index, caption in same_page_captions.iterrows():
        caption_bbox = caption['bbox']

        vertical_dist, horizontal_prox, vertical_direction = calculate_distance(table_bbox, caption_bbox)

        # Only consider captions that are either 'above' or 'below' the table and have a valid distance
        if vertical_dist != float('inf'):
            # Combine distances for a single proximity metric. Smaller is better.
            # We can give more weight to vertical distance if desired, but for now, simple sum.
            current_total_proximity = vertical_dist + abs(horizontal_prox)

            if current_total_proximity < min_total_proximity:
                min_total_proximity = current_total_proximity
                best_caption_for_table = caption
                best_caption_row_index = cap_row_index

    if best_caption_for_table is not None:
        # Recalculate exact vertical and horizontal for storage, as min_total_proximity is a combined metric
        final_vertical_dist, final_horizontal_prox, final_vertical_direction = calculate_distance(table_bbox, best_caption_for_table['bbox'])

        associated_table_caption_pairs.append({
            'table_page_number': table['page_number'],
            'table_bbox': table_bbox,
            'table_class_name': table['class_name'],
            'caption_page_number': best_caption_for_table['page_number'],
            'caption_bbox': best_caption_for_table['bbox'],
            'caption_class_name': best_caption_for_table['class_name'],
            'vertical_distance': final_vertical_dist,
            'horizontal_proximity': final_horizontal_prox,
            'vertical_direction': final_vertical_direction # Store if caption is above or below
        })
        # Remove the associated caption from available captions to prevent re-association
        available_table_captions_df = available_table_captions_df.drop(best_caption_row_index)

# Print a summary of the associated pairs
print("\nAssociated Table-Caption Pairs:")
if associated_table_caption_pairs:
    for i, pair in enumerate(associated_table_caption_pairs):
        print(f"Pair {i+1}:")
        print(f"  Table Page: {pair['table_page_number']}")
        print(f"  Table BBox: {pair['table_bbox']}")
        print(f"  Caption Page: {pair['caption_page_number']}")
        print(f"  Caption BBox: {pair['caption_bbox']}")
        print(f"  Vertical Distance: {pair['vertical_distance']:.2f}")
        print(f"  Horizontal Proximity: {pair['horizontal_proximity']:.2f}")
        print(f"  Vertical Direction: {pair['vertical_direction']}")
else:
    print("No table-caption pairs associated.")

print(f"\nSuccessfully associated {len(associated_table_caption_pairs)} table-caption pairs.")

"""## Crop and store all associated figure/table-caption pairs

### Subtask:
For each identified figure-caption and table-caption pair, crop the respective image regions (figure/table and its caption) from the correct page's image. Store these cropped combined images in a structured format.

**Reasoning**:
The first instruction is to create a list of all PDF page images by iterating through each page, converting it to an image, and storing it. This code block will achieve that.
"""

import fitz # PyMuPDF
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt # Import for pdf_to_image function if not already available

def pdf_to_image(pdf_path, page_number=0, dpi=200):
    doc = fitz.open(pdf_path)
    page = doc.load_page(page_number)
    pix = page.get_pixmap(matrix=fitz.Matrix(dpi / 72, dpi / 72))
    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
    doc.close()
    return np.array(img)

# Open the PDF document to get total_pages
doc = fitz.open(pdf_path)
total_pages = doc.page_count
doc.close()

print(f"Preparing images for {total_pages} pages...")
all_page_images = []
for page_num in range(total_pages):
    page_image = pdf_to_image(pdf_path, page_number=page_num, dpi=200)
    all_page_images.append(page_image)

print(f"Successfully created images for {len(all_page_images)} pages.")

"""**Reasoning**:
Now that all page images are prepared, the next step is to iterate through the associated figure-caption and table-caption pairs, crop the respective image regions, combine them, and store them along with their metadata.


"""

all_extracted_pairs = []

# Helper function to crop and combine image regions
def crop_and_combine(main_bbox, caption_bbox, page_image, pair_type, page_number):
    # Ensure bounding box coordinates are integers
    main_bbox = [int(coord) for coord in main_bbox]
    caption_bbox = [int(coord) for coord in caption_bbox]

    # Crop the main image (figure or table)
    main_x_min, main_y_min, main_x_max, main_y_max = main_bbox
    main_image = page_image[main_y_min:main_y_max, main_x_min:main_x_max]

    # Crop the caption image
    cap_x_min, cap_y_min, cap_x_max, cap_y_max = caption_bbox
    caption_image = page_image[cap_y_min:cap_y_max, cap_x_min:cap_x_max]

    # Get max width for padding to ensure consistent width for stacking
    max_width = max(main_image.shape[1], caption_image.shape[1])

    # Pad main image if its width is smaller
    if main_image.shape[1] < max_width:
        pad_width = max_width - main_image.shape[1]
        main_image = np.pad(main_image, ((0, 0), (0, pad_width), (0, 0)), mode='constant', constant_values=255)

    # Pad caption image if its width is smaller
    if caption_image.shape[1] < max_width:
        pad_width = max_width - caption_image.shape[1]
        caption_image = np.pad(caption_image, ((0, 0), (0, pad_width), (0, 0)), mode='constant', constant_values=255)

    # Combine main image and caption vertically, with a white separator
    combined_image = np.vstack((main_image, np.full((20, max_width, 3), 255), caption_image))

    return {
        'combined_image': combined_image,
        'pair_type': pair_type,
        'page_number': page_number,
        'main_bbox': main_bbox,
        'caption_bbox': caption_bbox
    }

# 3. Iterate through associated_figure_caption_pairs
print(f"Processing {len(associated_figure_caption_pairs)} figure-caption pairs...")
for pair in associated_figure_caption_pairs:
    figure_bbox = pair['figure_bbox']
    figure_page_number = pair['figure_page_number']
    caption_bbox = pair['caption_bbox']

    page_image = all_page_images[figure_page_number]

    extracted_pair_data = crop_and_combine(figure_bbox, caption_bbox, page_image, 'figure-caption', figure_page_number)
    all_extracted_pairs.append(extracted_pair_data)

# 4. Iterate through associated_table_caption_pairs
print(f"Processing {len(associated_table_caption_pairs)} table-caption pairs...")
for pair in associated_table_caption_pairs:
    table_bbox = pair['table_bbox']
    table_page_number = pair['table_page_number']
    caption_bbox = pair['caption_bbox']

    page_image = all_page_images[table_page_number]

    extracted_pair_data = crop_and_combine(table_bbox, caption_bbox, page_image, 'table-caption', table_page_number)
    all_extracted_pairs.append(extracted_pair_data)

# 5. Print a confirmation message
figure_caption_count = sum(1 for p in all_extracted_pairs if p['pair_type'] == 'figure-caption')
table_caption_count = sum(1 for p in all_extracted_pairs if p['pair_type'] == 'table-caption')

print(f"Successfully cropped and stored {figure_caption_count} figure-caption pairs and {table_caption_count} table-caption pairs.")
print(f"Total extracted pairs: {len(all_extracted_pairs)}")

"""**Reasoning**:
The final step of the subtask is to display a few of the extracted figure-caption and table-caption pairs to visually verify the cropping and combination process. This will fulfill the visualization requirement for the extracted structured format.


"""

print(f"Displaying {min(3, len(all_extracted_pairs))} extracted pairs...")
num_to_display = min(3, len(all_extracted_pairs))

for i in range(num_to_display):
    pair_data = all_extracted_pairs[i]
    plt.figure(figsize=(10, 10))
    plt.imshow(pair_data['combined_image'])
    plt.axis('off')
    plt.title(f"Extracted {pair_data['pair_type']} Pair {i+1} (Page {pair_data['page_number'] + 1})")
    plt.show()

print("Display complete.")

print(f"Displaying all extracted table-caption pairs...")

table_pair_count = 0
for pair_data in all_extracted_pairs:
    if pair_data['pair_type'] == 'table-caption':
        table_pair_count += 1
        plt.figure(figsize=(10, 10))
        plt.imshow(pair_data['combined_image'])
        plt.axis('off')
        plt.title(f"Extracted Table-Caption Pair {table_pair_count} (Page {pair_data['page_number'] + 1})")
        plt.show()

print(f"Total of {table_pair_count} table-caption pairs displayed.")

"""## Final Task

### Subtask:
Summarize the complete pipeline for extracting all figures and tables with their captions from the entire document, including the total number of extracted pairs and any notable observations.

## Summary:

### Data Analysis Key Findings

*   **Initial Detection Overview**: The YOLOv10 model processed 16 pages of the `HighlightedV1.pdf` document, resulting in a total of 234 detections.
*   **Object Classification and Counts**: The detections were classified into four main categories:
    *   3 figures
    *   4 figure captions
    *   13 tables
    *   11 table captions
*   **Figure-Caption Association**: A spatial proximity algorithm successfully associated 3 figure detections with 3 corresponding figure captions. All associated figure captions were found *below* their respective figures, exhibiting significant horizontal overlap.
*   **Table-Caption Association**: The spatial proximity algorithm also successfully associated 11 table detections with 11 table captions. Notably, table captions were found both *above* and *below* their associated tables, indicating a more varied placement compared to figure captions.
*   **Total Extracted Pairs**: In total, 14 combined pairs (3 figure-caption pairs and 11 table-caption pairs) were successfully extracted. Each pair consists of the cropped image of the figure/table vertically stacked with its associated caption.

### Insights or Next Steps

*   The implemented pipeline effectively extracts and associates figures/tables with their captions, providing a robust foundation for further content analysis from PDF documents. The distinction in caption placement (always below for figures, varied for tables) is a valuable observation for refining future models or parsing logic.
*   **Next Steps**: Perform Optical Character Recognition (OCR) on the extracted caption images to convert them into searchable text. This will enable semantic analysis of the document's visual content and facilitate building a comprehensive, searchable database of figures and tables.
"""